{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f84bd25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e917743",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. 데이터 준비\n",
    "# scikit-learn에서 Iris 데이터셋 불러오기\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59b4e159",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 데이터 표준화 (Normalization)\n",
    "# 신경망 학습 성능 향상을 위해 각 특성의 평균을 0, 분산을 1로 만듭니다.\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5366bce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 학습(train) 데이터와 테스트(test) 데이터로 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a905364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy 배열을 PyTorch 텐서로 변환\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67f1caa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5240c926",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. 모델 정의 (다층 퍼셉트론, MLP)\n",
    "# Iris 데이터는 4개의 특성(feature)을 가지므로 입력 레이어는 4개 노드,\n",
    "# 3개 클래스로 분류하므로 출력 레이어는 3개 노드를 가집니다.\n",
    "#IrisClassifier는 nn.Module을 상속받아 신경망 모델을 정의합니다\n",
    "\n",
    "class IrisClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IrisClassifier, self).__init__() #부모 생성자 호출하기\n",
    "        self.relu = nn.ReLU() #활성화 함수 \n",
    "        self.fc1 = nn.Linear(4, 16)  # 입력(4) -> 은닉층(16)\n",
    "        self.fc2 = nn.Linear(16, 32) # 은닉층(16) -> 은닉층(16)\n",
    "        self.fc2_2 = nn.Linear(32, 64) # 은닉층(16) -> 은닉층(16)\n",
    "        self.fc3 = nn.Linear(64, 3)  # 은닉층(16) -> 출력(3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)  #소프트맥스함수 필요 없음\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "49fcefb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델, 손실 함수, 옵티마이저 초기화\n",
    "model = IrisClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\"\"\"\n",
    "nn.CrossEntropyLoss는 다음과 같은 기능을 하나로 묶어 제공합니다.\n",
    "\n",
    "소프트맥스(Softmax): 모델의 최종 출력값(로짓, logit)을 \n",
    "각 클래스에 대한 확률 분포로 변환합니다.\n",
    "로그(Log): 확률값에 로그를 취합니다.\n",
    "음의 로그 가능도(Negative Log Likelihood) 손실 계산: 정답 클래스에 대한 확률의 로그값에 음수를 취해 손실을 계산합니다.\n",
    "\"\"\"\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6df84308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 모델 학습\n",
    "def train_model(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()  # 그라디언트 초기화\n",
    "            outputs = model(inputs) # 순전파\n",
    "            loss = criterion(outputs, labels) # 손실 계산\n",
    "            loss.backward() # 역전파\n",
    "            optimizer.step() # 가중치 업데이트\n",
    "        \n",
    "        # 에포크마다 손실 출력\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "    print('학습 완료!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "177edf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.8178\n",
      "Epoch [2/100], Loss: 0.3653\n",
      "Epoch [3/100], Loss: 0.2008\n",
      "Epoch [4/100], Loss: 0.1015\n",
      "Epoch [5/100], Loss: 0.0097\n",
      "Epoch [6/100], Loss: 0.0023\n",
      "Epoch [7/100], Loss: 0.0809\n",
      "Epoch [8/100], Loss: 0.0492\n",
      "Epoch [9/100], Loss: 0.0150\n",
      "Epoch [10/100], Loss: 0.0795\n",
      "Epoch [11/100], Loss: 0.0156\n",
      "Epoch [12/100], Loss: 0.0134\n",
      "Epoch [13/100], Loss: 0.0001\n",
      "Epoch [14/100], Loss: 0.0013\n",
      "Epoch [15/100], Loss: 0.0004\n",
      "Epoch [16/100], Loss: 0.0941\n",
      "Epoch [17/100], Loss: 0.0255\n",
      "Epoch [18/100], Loss: 0.0024\n",
      "Epoch [19/100], Loss: 0.2805\n",
      "Epoch [20/100], Loss: 0.0698\n",
      "Epoch [21/100], Loss: 0.0075\n",
      "Epoch [22/100], Loss: 0.0040\n",
      "Epoch [23/100], Loss: 0.1890\n",
      "Epoch [24/100], Loss: 0.5229\n",
      "Epoch [25/100], Loss: 0.0047\n",
      "Epoch [26/100], Loss: 0.0163\n",
      "Epoch [27/100], Loss: 0.1439\n",
      "Epoch [28/100], Loss: 0.2192\n",
      "Epoch [29/100], Loss: 0.0030\n",
      "Epoch [30/100], Loss: 0.0135\n",
      "Epoch [31/100], Loss: 0.0369\n",
      "Epoch [32/100], Loss: 0.0013\n",
      "Epoch [33/100], Loss: 0.0001\n",
      "Epoch [34/100], Loss: 0.0004\n",
      "Epoch [35/100], Loss: 0.0006\n",
      "Epoch [36/100], Loss: 0.0006\n",
      "Epoch [37/100], Loss: 0.0245\n",
      "Epoch [38/100], Loss: 0.1512\n",
      "Epoch [39/100], Loss: 0.0029\n",
      "Epoch [40/100], Loss: 0.0103\n",
      "Epoch [41/100], Loss: 0.0223\n",
      "Epoch [42/100], Loss: 0.0001\n",
      "Epoch [43/100], Loss: 0.0004\n",
      "Epoch [44/100], Loss: 0.0021\n",
      "Epoch [45/100], Loss: 0.0009\n",
      "Epoch [46/100], Loss: 0.0035\n",
      "Epoch [47/100], Loss: 0.0004\n",
      "Epoch [48/100], Loss: 0.0325\n",
      "Epoch [49/100], Loss: 0.0195\n",
      "Epoch [50/100], Loss: 0.0001\n",
      "Epoch [51/100], Loss: 0.0180\n",
      "Epoch [52/100], Loss: 0.0311\n",
      "Epoch [53/100], Loss: 0.0000\n",
      "Epoch [54/100], Loss: 0.0004\n",
      "Epoch [55/100], Loss: 0.1043\n",
      "Epoch [56/100], Loss: 0.0001\n",
      "Epoch [57/100], Loss: 0.0005\n",
      "Epoch [58/100], Loss: 0.0001\n",
      "Epoch [59/100], Loss: 0.0000\n",
      "Epoch [60/100], Loss: 0.4103\n",
      "Epoch [61/100], Loss: 0.0082\n",
      "Epoch [62/100], Loss: 0.1221\n",
      "Epoch [63/100], Loss: 0.3045\n",
      "Epoch [64/100], Loss: 0.0287\n",
      "Epoch [65/100], Loss: 0.1030\n",
      "Epoch [66/100], Loss: 0.0017\n",
      "Epoch [67/100], Loss: 0.0204\n",
      "Epoch [68/100], Loss: 0.0174\n",
      "Epoch [69/100], Loss: 0.0016\n",
      "Epoch [70/100], Loss: 0.0002\n",
      "Epoch [71/100], Loss: 0.3513\n",
      "Epoch [72/100], Loss: 0.0072\n",
      "Epoch [73/100], Loss: 0.0003\n",
      "Epoch [74/100], Loss: 0.0219\n",
      "Epoch [75/100], Loss: 0.0026\n",
      "Epoch [76/100], Loss: 0.0007\n",
      "Epoch [77/100], Loss: 0.0121\n",
      "Epoch [78/100], Loss: 0.0002\n",
      "Epoch [79/100], Loss: 0.0000\n",
      "Epoch [80/100], Loss: 0.0005\n",
      "Epoch [81/100], Loss: 0.0142\n",
      "Epoch [82/100], Loss: 0.0034\n",
      "Epoch [83/100], Loss: 0.0051\n",
      "Epoch [84/100], Loss: 0.0004\n",
      "Epoch [85/100], Loss: 0.0013\n",
      "Epoch [86/100], Loss: 0.0001\n",
      "Epoch [87/100], Loss: 0.0010\n",
      "Epoch [88/100], Loss: 0.0048\n",
      "Epoch [89/100], Loss: 0.0000\n",
      "Epoch [90/100], Loss: 0.0082\n",
      "Epoch [91/100], Loss: 0.4109\n",
      "Epoch [92/100], Loss: 0.0003\n",
      "Epoch [93/100], Loss: 0.1764\n",
      "Epoch [94/100], Loss: 0.1796\n",
      "Epoch [95/100], Loss: 0.0007\n",
      "Epoch [96/100], Loss: 0.0000\n",
      "Epoch [97/100], Loss: 0.0031\n",
      "Epoch [98/100], Loss: 0.0333\n",
      "Epoch [99/100], Loss: 0.0048\n",
      "Epoch [100/100], Loss: 0.0000\n",
      "학습 완료!\n",
      "훈련 데이터셋 정확도: 100.00%\n",
      "테스트 데이터셋 정확도: 100.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 4. 모델 평가 (학습 데이터셋 포함)\n",
    "def evaluate_model():\n",
    "    model.eval()  # 모델을 평가 모드로 전환\n",
    "    with torch.no_grad(): # 그라디언트 계산 비활성화\n",
    "        \n",
    "        # 학습 데이터셋 정확도 계산\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "        \n",
    "        accuracy_train = 100 * correct_train / total_train\n",
    "        print(f'훈련 데이터셋 정확도: {accuracy_train:.2f}%')\n",
    "        \n",
    "        # 테스트 데이터셋 정확도 계산\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "        \n",
    "        accuracy_test = 100 * correct_test / total_test\n",
    "        print(f'테스트 데이터셋 정확도: {accuracy_test:.2f}%')\n",
    "\n",
    "# 학습 및 평가 실행\n",
    "if __name__ == '__main__':\n",
    "    train_model(epochs=100)\n",
    "    evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e0c6e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b025a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85656190",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
