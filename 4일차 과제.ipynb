{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99c462aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle  #파일저장에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1c73dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#pip install tqdm\n",
    "from tqdm import tqdm # 학습 진행 상황 시각화를 위해 추가\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc0018c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 디바이스: cpu\n",
      "이미지 복사 및 폴더 생성 완료!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "# 이후 PyTorch, NumPy, TensorFlow 등을 import 합니다.\n",
    "# 경고 무시 설정\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. 경로 설정 및 파라미터\n",
    "original_dataset_dir = './data/cats_and_dogs/train'\n",
    "\"\"\"\n",
    "cats_and_dogs\n",
    "   ㄴ train \n",
    "\n",
    "cats_and_dogs_small\n",
    "   ㄴ train \n",
    "       ㄴcats\n",
    "       ㄴdogs \n",
    "   ㄴ test \n",
    "       ㄴcats\n",
    "       ㄴdogs \n",
    "   ㄴ validation \n",
    "        ㄴcats\n",
    "       ㄴdogs \n",
    "\n",
    "\"\"\"\n",
    "base_dir = './data/cats_and_dogs_small'\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test') #베이스 아래 3가지 폴더 생성\n",
    "\n",
    "model_save_path_pth = 'cats_and_dogs_model.pth'\n",
    "history_filepath = 'cats_and_dogs_history.pkl'\n",
    "\n",
    "batch_size = 16\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "num_epochs = 30 # 예시로 epoch 수를 30으로 설정했습니다.\n",
    "learning_rate = 0.001 # learning_rate 추가\n",
    "\n",
    "# GPU 사용 가능 여부 확인\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"사용 중인 디바이스: {device}\")\n",
    "\n",
    "# 이미지 복사 함수 \n",
    "def ImageCopyMove():\n",
    "    #경로가 있는지 확인해본다  경로가 있으면 True 없으면 False \n",
    "    if os.path.exists(base_dir): #베이스 아래\n",
    "        #shutil :shell util - 명령어 해석기 \n",
    "        shutil.rmtree(base_dir, ignore_errors=True)\n",
    "    os.makedirs(train_dir)  #디렉토리 생성 3개\n",
    "    os.makedirs(validation_dir)\n",
    "    os.makedirs(test_dir)\n",
    "    \n",
    "    #3개의 폴더에 고양이,강아지 파일 생성 하나씩\n",
    "    train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "    train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "    validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "    validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "    test_cats_dir = os.path.join(test_dir, 'cats')\n",
    "    test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
    "\n",
    "    os.makedirs(train_cats_dir)\n",
    "    os.makedirs(train_dogs_dir)\n",
    "    os.makedirs(validation_cats_dir)\n",
    "    os.makedirs(validation_dogs_dir)\n",
    "    os.makedirs(test_cats_dir)\n",
    "    os.makedirs(test_dogs_dir)\n",
    "    \n",
    "    #[\"cat.0.jpg\", \"cat.1.jpg\",..... ]\n",
    "    fnames = ['cat.{}.jpg'.format(i) for i in range(1000)] #이름 지정 트레인\n",
    "    for fname in fnames:\n",
    "        src = os.path.join(original_dataset_dir, fname)\n",
    "        dst = os.path.join(train_cats_dir, fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "        \n",
    "    fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)] #검증\n",
    "    for fname in fnames:\n",
    "        src = os.path.join(original_dataset_dir, fname)\n",
    "        dst = os.path.join(validation_cats_dir, fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "    \n",
    "    fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)] #테스트\n",
    "    for fname in fnames: \n",
    "        src = os.path.join(original_dataset_dir, fname)\n",
    "        dst = os.path.join(test_cats_dir, fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "    \n",
    "\n",
    "    # ---강아지로\n",
    "    \n",
    "    fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\n",
    "    for fname in fnames:\n",
    "        src = os.path.join(original_dataset_dir, fname)\n",
    "        dst = os.path.join(train_dogs_dir, fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "    \n",
    "    fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "    for fname in fnames:\n",
    "        src = os.path.join(original_dataset_dir, fname)\n",
    "        dst = os.path.join(validation_dogs_dir, fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "    \n",
    "    fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "    for fname in fnames:\n",
    "        src = os.path.join(original_dataset_dir, fname)\n",
    "        dst = os.path.join(test_dogs_dir, fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "\n",
    "    print(\"이미지 복사 및 폴더 생성 완료!\")\n",
    "\n",
    "\n",
    "ImageCopyMove()  \n",
    "\n",
    "#꽃분류_cnn 처럼 cnn분류하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1482cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "from PIL import Image as pilimg\n",
    "import imghdr\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1de4c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDate(folder,label,isTrain):\n",
    "    base_dir = \"./data/cats_and_dogs_small\"\n",
    "    if isTrain == 'train':\n",
    "        path = os.path.join(base_dir,\"train\",folder)\n",
    "    elif isTrain == 'test':\n",
    "        path = os.path.join(base_dir,\"test\",folder)\n",
    "    else:\n",
    "        raise ValueError('train,test,val,validation 중 하나여야 합니다.')\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    i = 1\n",
    "    for filename in os.listdir(path):\n",
    "        if i % 100 == 0:\n",
    "            print(i, filename)\n",
    "        i += 1\n",
    "        \n",
    "        file_path = os.path.join(path, filename)\n",
    "        try:\n",
    "            kind = imghdr.what(path + \"/\" + filename)\n",
    "            if kind in ['gif',\"png\",\"jpeg\",\"jpg\"]:\n",
    "                img = pilimg.open(path + '/' + filename).convert('RGB')\n",
    "                resize_img = img.resize((150,150))\n",
    "                pixel = np.array(resize_img)\n",
    "\n",
    "                if pixel.shape == (150, 150, 3):\n",
    "                    data.append(pixel)\n",
    "                    labels.append(label)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"{filename} error: {e}\")\n",
    "\n",
    "    data = np.array(data)       # 리스트 → 넘파이 배열\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    out_name = f\"imagedata{label}_{isTrain}.npz\"\n",
    "    np.savez(out_name, data=data, targets=labels)\n",
    "    print(out_name, '파일 저장 완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ed164de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#첫 1회\n",
    "def dataCreate():\n",
    "    cutes = ['dog','cat']\n",
    "    label = 0\n",
    "    for cute in cutes:\n",
    "        print(cute + '작업중....')\n",
    "        makeDate(cute,label,'train')\n",
    "        makeDate(cute,label,'test')\n",
    "        label += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d055f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_concat_data(mode, num_labels=2):\n",
    "    datas = []\n",
    "    targets = []\n",
    "    for label in range(num_labels):\n",
    "        \n",
    "        file_name = f\"imagedata{label}_{mode}.npz\"\n",
    "        npz_file =np.load(file_name)\n",
    "\n",
    "        datas.append(npz_file[\"data\"])\n",
    "        targets.append(npz_file[\"targets\"])\n",
    "\n",
    "    return np.concatenate(datas, axis= 0), np.concatenate(targets, axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b25b24b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Data_np,train_Target_np = load_and_concat_data('train',num_labels=2)\n",
    "test_Data_np,test_Target_np = load_and_concat_data('test',num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d55e5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#a( (1,2,3) )\n",
    "# 데이터 형태 변환 (PyTorch에 맞게)\n",
    "# PyTorch는 `channels_first` (C, H, W) 형태를 선호합니다.\n",
    "# 데이터 정규화: 0-255 -> 0.0-1.0\n",
    "# numpy => torch.tensor => dataset => dataloader\n",
    "#permute(0, 3, 1, 2) numpy => pytorch tensor 로 바꿀때 사용하는 함수 \n",
    "#permute - 차원순서 0 1 2 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab745e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData_tensor = (torch.from_numpy(train_Data_np)\n",
    "                    .float()\n",
    "                    .permute(0,3, 1, 2)\n",
    "                    / 255.0)\n",
    "\n",
    "testData_tensor = (torch.from_numpy(test_Data_np)\n",
    "                   .float()\n",
    "                   .permute(0,3, 1, 2)\n",
    "                    / 255.0 )\n",
    "\n",
    "#trainData_tensor = torch.from_numpy(trainData_np).float().permute(0, 3, 1, 2) / 255.0 .premute붙여서 오류\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1941d094",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTarget_tensor = torch.from_numpy(train_Target_np).long()\n",
    "testTarget_tensor = torch.from_numpy(test_Target_np).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe9eeecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: torch.Size([1278, 3, 150, 150])\n",
      "Test Data Shape: torch.Size([543, 3, 150, 150])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Data Shape: {trainData_tensor.shape}\")\n",
    "print(f\"Test Data Shape: {testData_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "355e0b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(trainData_tensor,trainTarget_tensor)\n",
    "test_dataset = TensorDataset(testData_tensor,testTarget_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset,batch_size=64, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0a9565e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Data Shape: torch.Size([1278, 3, 150, 150])\n",
    "#Test Data Shape: torch.Size([543, 3, 150, 150])\n",
    "\n",
    "class CuteLeNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CuteLeNN,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d( 3 ,32,kernel_size=3, stride=1,padding=1)\n",
    "        self.pool1 = nn.MaxPool2d( kernel_size=2,stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d( 32 ,64,kernel_size=3, stride=1,padding=1)\n",
    "        self.pool2 = nn.MaxPool2d( kernel_size=2,stride=2)\n",
    "\n",
    "        self.fc_input_size = 64 * 37* 37\n",
    "\n",
    "        self.fc1 = nn.Linear(self.fc_input_size,256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256,64)\n",
    "        self.fc3 = nn.Linear(64,32)\n",
    "        self.fc4 = nn.Linear(32,2) #2개\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = x.view(-1,self.fc_input_size)\n",
    "\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8cca5417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CuteLeNN(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=87616, out_features=256, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CuteLeNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7a62b65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion =nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7aaa323b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10,  Loss: 0.6903\n",
      "Epoch 2/10,  Loss: 0.6893\n",
      "Epoch 3/10,  Loss: 0.6887\n",
      "Epoch 4/10,  Loss: 0.6882\n",
      "Epoch 5/10,  Loss: 0.6878\n",
      "Epoch 6/10,  Loss: 0.6875\n",
      "Epoch 7/10,  Loss: 0.6872\n",
      "Epoch 8/10,  Loss: 0.6868\n",
      "Epoch 9/10,  Loss: 0.6863\n",
      "Epoch 10/10,  Loss: 0.6860\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs,labels) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs,labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch + 1}/{epochs},  Loss: {running_loss/len(train_loader):.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "46a1c61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "train_correct = 0\n",
    "train_total = 0\n",
    "test_correct = 0\n",
    "test_total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c4e15b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model(inputs)\n",
    "        _,predicted = torch.max(outputs.data,1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _,predicted = torch.max(outputs.data,1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "train_acc = 100 * train_correct / train_total\n",
    "test_acc = 100 * test_correct / test_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6ff0f848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "훈련셋 정확도: 55.24%\n",
      "테스트셋 정확도: 63.72%\n"
     ]
    }
   ],
   "source": [
    "print(f'\\n훈련셋 정확도: {train_acc:.2f}%')\n",
    "print(f'테스트셋 정확도: {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd978d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
