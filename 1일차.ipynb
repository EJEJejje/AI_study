{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b989e38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe4fcb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                5.1               3.5                1.4               0.2   \n",
      "1                4.9               3.0                1.4               0.2   \n",
      "2                4.7               3.2                1.3               0.2   \n",
      "3                4.6               3.1                1.5               0.2   \n",
      "4                5.0               3.6                1.4               0.2   \n",
      "\n",
      "   target  \n",
      "0       0  \n",
      "1       0  \n",
      "2       0  \n",
      "3       0  \n",
      "4       0  \n"
     ]
    }
   ],
   "source": [
    "# 1. 아이리스 데이터 로드 + 판다스 DataFrame 변환\n",
    "iris = load_iris()\n",
    "X = iris.data                          # (150, 4) 특징(feature)\n",
    "y = iris.target                        # (150,) 레이블(label: 0,1,2)\n",
    "\n",
    "df = pd.DataFrame(X, columns=iris.feature_names) #컬럼이름에 피쳐네임 지정\n",
    "df[\"target\"] = y #예측변수 지정\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c8277ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 학습/테스트 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[iris.feature_names],\n",
    "    df[\"target\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df[\"target\"], #타겟을 기준으로 비율 지정\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2574c5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 스케일링(Standardization)\n",
    "scaler = StandardScaler() #scaler에 스케일링함수 저장\n",
    "X_train_scaled = scaler.fit_transform(X_train) #X_train에 평균,편차 학습후 변환\n",
    "X_test_scaled = scaler.transform(X_test) #X_test 테스트에 스케일링 변환\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61960a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-28 15:47:27.635455: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2025-11-28 15:47:27.635515: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-11-28 15:47:27.635531: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.92 GB\n",
      "2025-11-28 15:47:27.635582: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-11-28 15:47:27.635599: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# 4. 딥러닝 모델 정의 (입력 4차원 데이터, 3층 신경망 은닉층 2개, 출력1개 3클래스)\n",
    "model = keras.Sequential([ #keras.Sequential 모델 지정\n",
    "    layers.Input(shape=(4,)),          # 꽃받침/꽃잎 길이·너비 4개 특성 인풋값에 특성4개 입력\n",
    "    layers.Dense(16, activation=\"relu\"), #히든레이어 1\n",
    "    layers.Dense(16, activation=\"relu\"), #히든레이어 2\n",
    "    layers.Dense(3, activation=\"softmax\")  # 출력층 클래스 3개\n",
    "])#Dense는 뉴런\n",
    "#차원 수에따라 히든레이어 지정후 1개는 출력층으로 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a249975",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. 모델 컴파일\n",
    "model.compile(\n",
    "    optimizer=\"adam\",                      # 옵티마이저(Optimizer) Adam 경사하강법 방법\n",
    "    loss=\"sparse_categorical_crossentropy\",# 손실 함수(Loss) – 정수 인코딩 스칼라값 손실함수.다중클래스 / \n",
    "    metrics=[\"accuracy\"]                   # 정확도(Accuracy) 측정\n",
    ")\n",
    "\n",
    "# 6. EarlyStopping 콜백(과적합 방지)\n",
    "early_stop = keras.callbacks.EarlyStopping( #keras모델이 검증로스 기준으로 epoch마다 재 계산\n",
    "    monitor=\"val_loss\", #과적합 감지를 검증로스로 보겠다.\n",
    "    patience=10, #개선되지않는 에폭 횟수 20번나오면 멈춰라. \n",
    "    restore_best_weights=True #그중 성능이 가장좋았던모델 선택\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e26bd150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-28 15:55:38.261754: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - accuracy: 0.6042 - loss: 0.7706 - val_accuracy: 0.5833 - val_loss: 0.7306\n",
      "Epoch 2/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7396 - loss: 0.5991 - val_accuracy: 0.5833 - val_loss: 0.6182\n",
      "Epoch 3/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8021 - loss: 0.5090 - val_accuracy: 0.7083 - val_loss: 0.5518\n",
      "Epoch 4/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8333 - loss: 0.4521 - val_accuracy: 0.7500 - val_loss: 0.5068\n",
      "Epoch 5/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8542 - loss: 0.4164 - val_accuracy: 0.7917 - val_loss: 0.4704\n",
      "Epoch 6/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8646 - loss: 0.3886 - val_accuracy: 0.7917 - val_loss: 0.4435\n",
      "Epoch 7/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8646 - loss: 0.3683 - val_accuracy: 0.8333 - val_loss: 0.4180\n",
      "Epoch 8/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8750 - loss: 0.3521 - val_accuracy: 0.8750 - val_loss: 0.3965\n",
      "Epoch 9/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8750 - loss: 0.3390 - val_accuracy: 0.8750 - val_loss: 0.3809\n",
      "Epoch 10/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8750 - loss: 0.3235 - val_accuracy: 0.8750 - val_loss: 0.3676\n",
      "Epoch 11/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8854 - loss: 0.3119 - val_accuracy: 0.9167 - val_loss: 0.3536\n",
      "Epoch 12/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8958 - loss: 0.3030 - val_accuracy: 0.9167 - val_loss: 0.3395\n",
      "Epoch 13/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8958 - loss: 0.2916 - val_accuracy: 0.9167 - val_loss: 0.3282\n",
      "Epoch 14/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8750 - loss: 0.2836 - val_accuracy: 0.9167 - val_loss: 0.3230\n",
      "Epoch 15/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8854 - loss: 0.2744 - val_accuracy: 0.9167 - val_loss: 0.3092\n",
      "Epoch 16/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9062 - loss: 0.2668 - val_accuracy: 0.9167 - val_loss: 0.2945\n",
      "Epoch 17/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9062 - loss: 0.2594 - val_accuracy: 0.9167 - val_loss: 0.2837\n",
      "Epoch 18/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9062 - loss: 0.2528 - val_accuracy: 0.9167 - val_loss: 0.2752\n",
      "Epoch 19/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9062 - loss: 0.2463 - val_accuracy: 0.9167 - val_loss: 0.2574\n",
      "Epoch 20/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9062 - loss: 0.2397 - val_accuracy: 0.9167 - val_loss: 0.2466\n",
      "Epoch 21/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9062 - loss: 0.2309 - val_accuracy: 0.9167 - val_loss: 0.2419\n",
      "Epoch 22/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9062 - loss: 0.2233 - val_accuracy: 0.9167 - val_loss: 0.2320\n",
      "Epoch 23/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9062 - loss: 0.2169 - val_accuracy: 0.9583 - val_loss: 0.2254\n",
      "Epoch 24/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9167 - loss: 0.2090 - val_accuracy: 0.9583 - val_loss: 0.2120\n",
      "Epoch 25/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9271 - loss: 0.2040 - val_accuracy: 0.9583 - val_loss: 0.1969\n",
      "Epoch 26/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9271 - loss: 0.2000 - val_accuracy: 0.9583 - val_loss: 0.1947\n",
      "Epoch 27/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9271 - loss: 0.1912 - val_accuracy: 0.9583 - val_loss: 0.1821\n",
      "Epoch 28/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9375 - loss: 0.1842 - val_accuracy: 0.9583 - val_loss: 0.1712\n",
      "Epoch 29/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9375 - loss: 0.1783 - val_accuracy: 0.9583 - val_loss: 0.1630\n",
      "Epoch 30/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9375 - loss: 0.1736 - val_accuracy: 0.9583 - val_loss: 0.1527\n",
      "Epoch 31/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9375 - loss: 0.1700 - val_accuracy: 0.9583 - val_loss: 0.1523\n",
      "Epoch 32/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9375 - loss: 0.1650 - val_accuracy: 1.0000 - val_loss: 0.1362\n",
      "Epoch 33/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9479 - loss: 0.1571 - val_accuracy: 0.9583 - val_loss: 0.1346\n",
      "Epoch 34/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9479 - loss: 0.1516 - val_accuracy: 1.0000 - val_loss: 0.1273\n",
      "Epoch 35/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9479 - loss: 0.1462 - val_accuracy: 1.0000 - val_loss: 0.1199\n",
      "Epoch 36/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9479 - loss: 0.1456 - val_accuracy: 1.0000 - val_loss: 0.1092\n",
      "Epoch 37/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9479 - loss: 0.1376 - val_accuracy: 1.0000 - val_loss: 0.1050\n",
      "Epoch 38/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9583 - loss: 0.1334 - val_accuracy: 1.0000 - val_loss: 0.1024\n",
      "Epoch 39/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9583 - loss: 0.1296 - val_accuracy: 1.0000 - val_loss: 0.0946\n",
      "Epoch 40/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9583 - loss: 0.1273 - val_accuracy: 1.0000 - val_loss: 0.0902\n",
      "Epoch 41/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9583 - loss: 0.1225 - val_accuracy: 1.0000 - val_loss: 0.0855\n",
      "Epoch 42/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9583 - loss: 0.1193 - val_accuracy: 1.0000 - val_loss: 0.0802\n",
      "Epoch 43/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9583 - loss: 0.1177 - val_accuracy: 1.0000 - val_loss: 0.0768\n",
      "Epoch 44/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9583 - loss: 0.1144 - val_accuracy: 1.0000 - val_loss: 0.0729\n",
      "Epoch 45/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9583 - loss: 0.1105 - val_accuracy: 1.0000 - val_loss: 0.0688\n",
      "Epoch 46/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9583 - loss: 0.1093 - val_accuracy: 1.0000 - val_loss: 0.0653\n",
      "Epoch 47/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9583 - loss: 0.1059 - val_accuracy: 1.0000 - val_loss: 0.0639\n",
      "Epoch 48/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.1029 - val_accuracy: 1.0000 - val_loss: 0.0606\n",
      "Epoch 49/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9583 - loss: 0.1041 - val_accuracy: 1.0000 - val_loss: 0.0577\n",
      "Epoch 50/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9583 - loss: 0.0999 - val_accuracy: 1.0000 - val_loss: 0.0561\n",
      "Epoch 51/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.0975 - val_accuracy: 1.0000 - val_loss: 0.0532\n",
      "Epoch 52/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.0961 - val_accuracy: 1.0000 - val_loss: 0.0512\n",
      "Epoch 53/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9583 - loss: 0.0969 - val_accuracy: 1.0000 - val_loss: 0.0489\n",
      "Epoch 54/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.0938 - val_accuracy: 1.0000 - val_loss: 0.0472\n",
      "Epoch 55/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.0903 - val_accuracy: 1.0000 - val_loss: 0.0454\n",
      "Epoch 56/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.0890 - val_accuracy: 1.0000 - val_loss: 0.0437\n",
      "Epoch 57/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9583 - loss: 0.0896 - val_accuracy: 1.0000 - val_loss: 0.0426\n",
      "Epoch 58/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.0873 - val_accuracy: 1.0000 - val_loss: 0.0407\n",
      "Epoch 59/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9688 - loss: 0.0849 - val_accuracy: 1.0000 - val_loss: 0.0398\n",
      "Epoch 60/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.0847 - val_accuracy: 1.0000 - val_loss: 0.0381\n",
      "Epoch 61/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.0819 - val_accuracy: 1.0000 - val_loss: 0.0375\n",
      "Epoch 62/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.0829 - val_accuracy: 1.0000 - val_loss: 0.0359\n",
      "Epoch 63/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9688 - loss: 0.0802 - val_accuracy: 1.0000 - val_loss: 0.0348\n",
      "Epoch 64/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.0796 - val_accuracy: 1.0000 - val_loss: 0.0338\n",
      "Epoch 65/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.0784 - val_accuracy: 1.0000 - val_loss: 0.0334\n",
      "Epoch 66/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.0780 - val_accuracy: 1.0000 - val_loss: 0.0318\n",
      "Epoch 67/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.0776 - val_accuracy: 1.0000 - val_loss: 0.0305\n",
      "Epoch 68/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9688 - loss: 0.0768 - val_accuracy: 1.0000 - val_loss: 0.0306\n",
      "Epoch 69/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.0792 - val_accuracy: 1.0000 - val_loss: 0.0290\n",
      "Epoch 70/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.0732 - val_accuracy: 1.0000 - val_loss: 0.0291\n",
      "Epoch 71/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.0734 - val_accuracy: 1.0000 - val_loss: 0.0288\n",
      "Epoch 72/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9688 - loss: 0.0761 - val_accuracy: 1.0000 - val_loss: 0.0278\n",
      "Epoch 73/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.0716 - val_accuracy: 1.0000 - val_loss: 0.0264\n",
      "Epoch 74/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.0735 - val_accuracy: 1.0000 - val_loss: 0.0263\n",
      "Epoch 75/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9688 - loss: 0.0746 - val_accuracy: 1.0000 - val_loss: 0.0268\n",
      "Epoch 76/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.0691 - val_accuracy: 1.0000 - val_loss: 0.0245\n",
      "Epoch 77/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9583 - loss: 0.0725 - val_accuracy: 1.0000 - val_loss: 0.0239\n",
      "Epoch 78/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9688 - loss: 0.0685 - val_accuracy: 1.0000 - val_loss: 0.0258\n",
      "Epoch 79/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9688 - loss: 0.0699 - val_accuracy: 1.0000 - val_loss: 0.0239\n",
      "Epoch 80/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.0684 - val_accuracy: 1.0000 - val_loss: 0.0230\n",
      "Epoch 81/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.0680 - val_accuracy: 1.0000 - val_loss: 0.0231\n",
      "Epoch 82/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9688 - loss: 0.0705 - val_accuracy: 1.0000 - val_loss: 0.0261\n",
      "Epoch 83/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.0661 - val_accuracy: 1.0000 - val_loss: 0.0219\n",
      "Epoch 84/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.0671 - val_accuracy: 1.0000 - val_loss: 0.0213\n",
      "Epoch 85/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.0684 - val_accuracy: 1.0000 - val_loss: 0.0227\n",
      "Epoch 86/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9688 - loss: 0.0673 - val_accuracy: 1.0000 - val_loss: 0.0209\n",
      "Epoch 87/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.0656 - val_accuracy: 1.0000 - val_loss: 0.0202\n",
      "Epoch 88/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9688 - loss: 0.0661 - val_accuracy: 1.0000 - val_loss: 0.0196\n",
      "Epoch 89/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9688 - loss: 0.0666 - val_accuracy: 1.0000 - val_loss: 0.0229\n",
      "Epoch 90/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9688 - loss: 0.0666 - val_accuracy: 1.0000 - val_loss: 0.0211\n",
      "Epoch 91/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.0710 - val_accuracy: 1.0000 - val_loss: 0.0188\n",
      "Epoch 92/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9792 - loss: 0.0687 - val_accuracy: 1.0000 - val_loss: 0.0222\n",
      "Epoch 93/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.0671 - val_accuracy: 1.0000 - val_loss: 0.0183\n",
      "Epoch 94/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9688 - loss: 0.0628 - val_accuracy: 1.0000 - val_loss: 0.0191\n",
      "Epoch 95/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9688 - loss: 0.0628 - val_accuracy: 1.0000 - val_loss: 0.0192\n",
      "Epoch 96/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.0670 - val_accuracy: 1.0000 - val_loss: 0.0172\n",
      "Epoch 97/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.0630 - val_accuracy: 1.0000 - val_loss: 0.0202\n",
      "Epoch 98/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.0655 - val_accuracy: 1.0000 - val_loss: 0.0177\n",
      "Epoch 99/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9688 - loss: 0.0609 - val_accuracy: 1.0000 - val_loss: 0.0188\n",
      "Epoch 100/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.0628 - val_accuracy: 1.0000 - val_loss: 0.0179\n"
     ]
    }
   ],
   "source": [
    "# 7. 모델 학습\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,      # 학습 데이터의 20%를 검증(Validation)에 사용\n",
    "    epochs=100, #200번 반복\n",
    "    batch_size=6, # 전체 데이터를 /16으로 나눠서 \n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11fa1c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0711, Test Accuracy: 0.9667\n"
     ]
    }
   ],
   "source": [
    "# 8. 테스트 데이터로 평가\n",
    "test_loss, test_acc = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa72f749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n",
      "실제 레이블: [0 2 1 1 0]\n",
      "예측 레이블: [0 2 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 9. 예측(Prediction) 예시\n",
    "sample = X_test_scaled[:5]\n",
    "pred_prob = model.predict(sample)          # 각 클래스 확률\n",
    "pred_class = np.argmax(pred_prob, axis=1)  # 확률이 가장 큰 클래스 선택\n",
    "\n",
    "print(\"실제 레이블:\", y_test.to_numpy()[:5])\n",
    "print(\"예측 레이블:\", pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae6cae5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a45da4",
   "metadata": {},
   "source": [
    "# 파이토치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0faf5aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.8732\n",
      "Epoch [2/100], Loss: 0.4511\n",
      "Epoch [3/100], Loss: 0.4772\n",
      "Epoch [4/100], Loss: 0.2857\n",
      "Epoch [5/100], Loss: 0.3230\n",
      "Epoch [6/100], Loss: 0.2374\n",
      "Epoch [7/100], Loss: 0.0931\n",
      "Epoch [8/100], Loss: 0.0773\n",
      "Epoch [9/100], Loss: 0.3841\n",
      "Epoch [10/100], Loss: 0.0527\n",
      "Epoch [11/100], Loss: 0.2242\n",
      "Epoch [12/100], Loss: 0.0287\n",
      "Epoch [13/100], Loss: 0.0029\n",
      "Epoch [14/100], Loss: 0.0140\n",
      "Epoch [15/100], Loss: 0.3733\n",
      "Epoch [16/100], Loss: 0.0501\n",
      "Epoch [17/100], Loss: 0.0175\n",
      "Epoch [18/100], Loss: 0.1803\n",
      "Epoch [19/100], Loss: 0.1651\n",
      "Epoch [20/100], Loss: 0.0736\n",
      "Epoch [21/100], Loss: 0.0002\n",
      "Epoch [22/100], Loss: 0.0009\n",
      "Epoch [23/100], Loss: 0.0417\n",
      "Epoch [24/100], Loss: 0.0007\n",
      "Epoch [25/100], Loss: 0.4121\n",
      "Epoch [26/100], Loss: 0.0613\n",
      "Epoch [27/100], Loss: 0.0096\n",
      "Epoch [28/100], Loss: 0.0039\n",
      "Epoch [29/100], Loss: 0.0684\n",
      "Epoch [30/100], Loss: 0.4786\n",
      "Epoch [31/100], Loss: 0.1932\n",
      "Epoch [32/100], Loss: 0.3487\n",
      "Epoch [33/100], Loss: 0.0068\n",
      "Epoch [34/100], Loss: 0.1021\n",
      "Epoch [35/100], Loss: 0.0023\n",
      "Epoch [36/100], Loss: 0.0020\n",
      "Epoch [37/100], Loss: 0.1817\n",
      "Epoch [38/100], Loss: 0.0023\n",
      "Epoch [39/100], Loss: 0.0074\n",
      "Epoch [40/100], Loss: 0.0117\n",
      "Epoch [41/100], Loss: 0.0261\n",
      "Epoch [42/100], Loss: 0.0008\n",
      "Epoch [43/100], Loss: 0.0448\n",
      "Epoch [44/100], Loss: 0.3155\n",
      "Epoch [45/100], Loss: 0.1321\n",
      "Epoch [46/100], Loss: 0.0023\n",
      "Epoch [47/100], Loss: 0.1007\n",
      "Epoch [48/100], Loss: 0.0004\n",
      "Epoch [49/100], Loss: 0.0008\n",
      "Epoch [50/100], Loss: 0.0009\n",
      "Epoch [51/100], Loss: 0.0455\n",
      "Epoch [52/100], Loss: 0.0006\n",
      "Epoch [53/100], Loss: 0.0365\n",
      "Epoch [54/100], Loss: 0.0096\n",
      "Epoch [55/100], Loss: 0.0127\n",
      "Epoch [56/100], Loss: 0.0005\n",
      "Epoch [57/100], Loss: 0.0012\n",
      "Epoch [58/100], Loss: 0.0020\n",
      "Epoch [59/100], Loss: 0.0026\n",
      "Epoch [60/100], Loss: 0.1681\n",
      "Epoch [61/100], Loss: 0.3371\n",
      "Epoch [62/100], Loss: 0.0379\n",
      "Epoch [63/100], Loss: 0.0005\n",
      "Epoch [64/100], Loss: 0.0006\n",
      "Epoch [65/100], Loss: 0.0104\n",
      "Epoch [66/100], Loss: 0.0770\n",
      "Epoch [67/100], Loss: 0.0003\n",
      "Epoch [68/100], Loss: 0.0307\n",
      "Epoch [69/100], Loss: 0.0004\n",
      "Epoch [70/100], Loss: 0.0040\n",
      "Epoch [71/100], Loss: 0.0366\n",
      "Epoch [72/100], Loss: 0.0003\n",
      "Epoch [73/100], Loss: 0.0051\n",
      "Epoch [74/100], Loss: 0.1818\n",
      "Epoch [75/100], Loss: 0.0780\n",
      "Epoch [76/100], Loss: 0.0330\n",
      "Epoch [77/100], Loss: 0.0033\n",
      "Epoch [78/100], Loss: 0.0033\n",
      "Epoch [79/100], Loss: 0.0113\n",
      "Epoch [80/100], Loss: 0.0033\n",
      "Epoch [81/100], Loss: 0.0079\n",
      "Epoch [82/100], Loss: 0.0032\n",
      "Epoch [83/100], Loss: 0.0121\n",
      "Epoch [84/100], Loss: 0.0002\n",
      "Epoch [85/100], Loss: 0.0020\n",
      "Epoch [86/100], Loss: 0.0006\n",
      "Epoch [87/100], Loss: 0.0001\n",
      "Epoch [88/100], Loss: 0.0198\n",
      "Epoch [89/100], Loss: 0.0002\n",
      "Epoch [90/100], Loss: 0.0002\n",
      "Epoch [91/100], Loss: 0.0049\n",
      "Epoch [92/100], Loss: 0.0057\n",
      "Epoch [93/100], Loss: 0.0665\n",
      "Epoch [94/100], Loss: 0.0001\n",
      "Epoch [95/100], Loss: 0.0013\n",
      "Epoch [96/100], Loss: 0.0257\n",
      "Epoch [97/100], Loss: 0.0026\n",
      "Epoch [98/100], Loss: 0.0741\n",
      "Epoch [99/100], Loss: 0.0616\n",
      "Epoch [100/100], Loss: 0.0001\n",
      "학습 완료!\n",
      "훈련 데이터셋 정확도: 99.17%\n",
      "테스트 데이터셋 정확도: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터 준비\n",
    "# scikit-learn에서 Iris 데이터셋 불러오기\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 데이터 표준화 (Normalization)\n",
    "# 신경망 학습 성능 향상을 위해 각 특성의 평균을 0, 분산을 1로 만듭니다.\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X) #동일하게 표준편차 기준 평균 학습후 변환\n",
    "\n",
    "# 학습(train) 데이터와 테스트(test) 데이터로 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# NumPy 배열을 PyTorch 텐서로 변환\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long) #int 64 로옹~\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# 텐서 데이터셋 및 데이터로더 생성\n",
    "#NumPy 배열인 데이터를 PyTorch가 처리할 수 있는 torch.tensor로 변환하고, \n",
    "# 이를 TensorDataset과 DataLoader로 묶습니다. DataLoader는 데이터를 \n",
    "# 미니배치(mini-batch) 단위로 묶어 효율적으로 모델에 전달하는 역할을 합니다.\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor) #넘파이로 데이터 전달\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True) #총 데이터셋을 16으로 나눔 150/16 번\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# 2. 모델 정의 (다층 퍼셉트론, MLP)\n",
    "# Iris 데이터는 4개의 특성(feature)을 가지므로 입력 레이어는 4개 노드,\n",
    "# 3개 클래스로 분류하므로 출력 레이어는 3개 노드를 가집니다.\n",
    "#IrisClassifier는 nn.Module을 상속받아 신경망 모델을 정의합니다\n",
    "\n",
    "class IrisClassifier(nn.Module): #아이리스 분류모델 클래스 지정.\n",
    "    def __init__(self):\n",
    "        super(IrisClassifier, self).__init__() #부모 생성자 호출하기\n",
    "        self.fc1 = nn.Linear(4, 16)  # 입력(4) -> 은닉층(16)\n",
    "        self.relu = nn.ReLU() #활성화 함수 > 이후 선형으로 분류\n",
    "        self.fc2 = nn.Linear(16, 16) # 은닉층(16) -> 은닉층(16)\n",
    "        self.fc3 = nn.Linear(16, 3)  # 은닉층(16) -> 출력(3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)  #소프트맥스함수 필요 없음\n",
    "        return x #3종류의 클래스점수 로짓\n",
    "\n",
    "# 모델, 손실 함수, 옵티마이저 초기화\n",
    "model = IrisClassifier()\n",
    "criterion = nn.CrossEntropyLoss() #소프트맥스,크로스엔트로피 교차 적용\n",
    "\"\"\"\n",
    "nn.CrossEntropyLoss는 다음과 같은 기능을 하나로 묶어 제공합니다.\n",
    "\n",
    "소프트맥스(Softmax): 모델의 최종 출력값(로짓, logit)을 \n",
    "각 클래스에 대한 확률 분포로 변환합니다.\n",
    "로그(Log): 확률값에 로그를 취합니다.\n",
    "음의 로그 가능도(Negative Log Likelihood) 손실 계산: 정답 클래스에 대한 확률의 로그값에 음수를 취해 손실을 계산합니다.\n",
    "\"\"\"\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)#경사하강법 아담로직\n",
    "\n",
    "# 3. 모델 학습\n",
    "def train_model(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, labels in train_loader: #트레인 데이터셋에 대하여 인풋과 라벨을지정.\n",
    "            optimizer.zero_grad()  # 그라디언트 초기화\n",
    "            outputs = model(inputs) # 순전파 모델은 클래스\n",
    "            loss = criterion(outputs, labels) # 손실 계산 크로스엔트로로피로\n",
    "            loss.backward() # 역전파\n",
    "            optimizer.step() # 가중치 업데이트\n",
    "        \n",
    "        # 에포크마다 손실 출력\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "    print('학습 완료!')\n",
    "\n",
    "# 4. 모델 평가 (학습 데이터셋 포함)\n",
    "def evaluate_model():\n",
    "    model.eval()  # 모델을 평가 모드로 전환\n",
    "    with torch.no_grad(): # 그라디언트 계산 비활성화\n",
    "        \n",
    "        # 학습 데이터셋 정확도 계산\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for inputs, labels in train_loader:  #train_loader 안에서 \n",
    "            outputs = model(inputs) #모델 입력값\n",
    "            _, predicted = torch.max(outputs.data, 1) #_언더스코어 최댓값버리고 predicted 가장 큰 예측값저장.\n",
    "            total_train += labels.size(0) #훈련수\n",
    "            correct_train += (predicted == labels).sum().item() #모델 인풋값을 꺼내서  예측시 라벨과 맞는값일경우 전부 더해서  correct_train에 저장\n",
    "        \n",
    "        accuracy_train = 100 * correct_train / total_train #비율\n",
    "        print(f'훈련 데이터셋 정확도: {accuracy_train:.2f}%')\n",
    "        \n",
    "        # 테스트 데이터셋 정확도 계산\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "        \n",
    "        accuracy_test = 100 * correct_test / total_test\n",
    "        print(f'테스트 데이터셋 정확도: {accuracy_test:.2f}%')\n",
    "\n",
    "# 학습 및 평가 실행\n",
    "if __name__ == '__main__':\n",
    "    train_model(epochs=100)\n",
    "    evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c576d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
